{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 83243,
     "status": "error",
     "timestamp": 1754058146090,
     "user": {
      "displayName": "Jayant Dahiya",
      "userId": "07207346439348444270"
     },
     "user_tz": -330
    },
    "id": "ctK1p6V0_c4Z",
    "outputId": "77c647ae-4245-4d57-bbc1-0016479f34ad"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üìà Professional Stock Market Prediction with LSTM\n",
    "\n",
    "This notebook provides a complete, production-ready guide to implementing a sophisticated stock prediction model using Long Short-Term Memory (LSTM) networks. The methodology is synthesized from the findings of three key research papers, focusing on the highest-performing architecture and techniques to create a practical and robust tool.\n",
    "\n",
    "[cite_start]**Core Methodology:** The implementation is primarily based on the work of **Shobayo et al. (2025)**, who achieved an **R-squared of 0.993** and a Mean Absolute Percentage Error (MAPE) of 1.33% with an optimized LSTM model[cite: 220, 221]. [cite_start]This model uses a 60-day time step and incorporates On-Balance Volume (OBV) as a key feature[cite: 221].\n",
    "\n",
    "### Key Features of this Notebook:\n",
    "* [cite_start]**Interactive & Customizable:** An interactive widget allows you to easily change the stock ticker and other key parameters[cite: 183].\n",
    "* **End-to-End Pipeline:** Covers the entire workflow from data acquisition and feature engineering to model training, evaluation, and visualization.\n",
    "* **Automated Evaluation:** Automatically evaluates the LSTM model against a naive baseline to provide a clear performance benchmark.\n",
    "* **Robust & Reliable:** Includes input validation, error handling, and clear success/failure feedback for a smooth user experience.\n",
    "* **Practical Outputs:** Provides functionality to save the trained model, load it for future use, and export predictions to a CSV file.\n",
    "\n",
    "### Theoretical Backing (Adaptive Markets Hypothesis)\n",
    "[cite_start]While the traditional Efficient Market Hypothesis (EMH) suggests that market prices are unpredictable, this notebook's approach is better supported by the **Adaptive Markets Hypothesis (AMH)**[cite: 158]. [cite_start]The AMH posits that markets are not perfectly efficient and that temporary inefficiencies arise, which advanced machine learning models like LSTM can learn to exploit for prediction[cite: 159].\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# @title\n",
    "# # üìö Table of Contents\n",
    "#\n",
    "# 1.  [Setup & Dependencies](#section1)\n",
    "# 2.  [Configuration & Parameters](#section2)\n",
    "# 3.  [Data Acquisition & Exploration](#section3)\n",
    "# 4.  [Feature Engineering (Technical Indicators)](#section4)\n",
    "# 5.  [Data Preprocessing](#section5)\n",
    "# 6.  [LSTM Model Implementation](#section6)\n",
    "# 7.  [Model Training](#section7)\n",
    "# 8.  [Prediction & Performance Evaluation](#section8)\n",
    "#     * [8.1 Baseline Model Comparison](#subsection8_1)\n",
    "#     * [8.2 Visualizing Results](#subsection8_2)\n",
    "#     * [8.3 Exporting Predictions](#subsection8_3)\n",
    "# 9.  [Saving and Loading the Model](#section9)\n",
    "# 10. [Live Prediction on New Data](#section10)\n",
    "# 11. [Advanced Topics & Future Work](#section11)\n",
    "#\n",
    "# ---\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section1\"></a>\n",
    "# ## 1. Setup & Dependencies\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown We are installing and importing all the necessary Python libraries for our project.\n",
    "# [cite_start]@markdown - `yfinance`: To fetch historical stock market data from Yahoo Finance[cite: 106, 488].\n",
    "# [cite_start]@markdown - `pandas` & `numpy`: For data manipulation and numerical operations[cite: 129, 418, 419].\n",
    "# [cite_start]@markdown - `scikit-learn`: For data preprocessing (scaling) and evaluation metrics[cite: 129, 420].\n",
    "# [cite_start]@markdown - `tensorflow`: The deep learning framework to build and train our LSTM model[cite: 129].\n",
    "# @markdown - `plotly`: To create interactive and professional-looking visualizations.\n",
    "# @markdown\n",
    "# [cite_start]@markdown The code block also checks for GPU availability to accelerate model training, a key performance optimization technique mentioned in the research[cite: 459].\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown **Run this cell to install and import dependencies.**\n",
    "\n",
    "# Install libraries\n",
    "!pip install yfinance pandas numpy scikit-learn tensorflow plotly kaleido -q\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from google.colab import drive, files\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# --- GPU Check ---\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('‚ö†Ô∏è GPU device not found. Training will be on CPU.')\n",
    "else:\n",
    "  print(f'‚úÖ Found GPU at: {device_name}. Training will be GPU-accelerated.')\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "print(\"‚úÖ Dependencies installed and imported successfully.\")\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section2\"></a>\n",
    "# ## 2. Configuration & Parameters\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown This interactive form centralizes all key parameters, making the notebook highly reusable and easy to customize. The default values are set to the **highest-performing configuration** identified in the research by Shobayo et al. (2025).\n",
    "# @markdown\n",
    "# [cite_start]@markdown - **Model:** Optimised LSTM (60-Day Time Step with OBV)[cite: 221, 303].\n",
    "# @markdown - **Stock:** Enter any valid Yahoo Finance ticker symbol. `AAPL` is the default.\n",
    "# [cite_start]@markdown - **Date Range:** A long period provides more data for robust training[cite: 512].\n",
    "# [cite_start]@markdown - **Window Size:** Set to 60, found to be empirically optimal in multiple studies[cite: 62, 270, 341].\n",
    "# [cite_start]@markdown - **Features:** The list includes `Close` and `OBV`, the combination that yielded an R¬≤ of 0.993[cite: 221].\n",
    "# [cite_start]@markdown - **Hyperparameters:** The values for units, dropout rate, and learning rate are taken directly from the optimized model specifications[cite: 303].\n",
    "\n",
    "# --- General Configuration ---\n",
    "STOCK_TICKER = \"AAPL\" # @param {type:\"string\"}\n",
    "START_DATE = \"2010-01-01\" # @param {type:\"date\"}\n",
    "END_DATE = \"2024-06-07\" # @param {type:\"date\"}\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.85 # @param {type:\"slider\", min:0.7, max:0.9, step:0.05}\n",
    "\n",
    "# --- Model & Feature Configuration ---\n",
    "WINDOW_SIZE = 60 # @param {type:\"integer\"}\n",
    "# [cite_start]The best model used 'Close' and 'OBV'[cite: 221]. You can add more features here.\n",
    "# e.g., ['Close', 'OBV', 'RSI', 'MACD']\n",
    "FEATURES_TO_USE = ['Close', 'OBV']\n",
    "\n",
    "# --- LSTM Hyperparameters (from Shobayo et al., 2025) ---\n",
    "LSTM_UNITS = 104 # @param {type:\"integer\"}\n",
    "DROPOUT_RATE = 0.266 # @param {type:\"number\"}\n",
    "LEARNING_RATE = 0.00369 # @param {type:\"number\"}\n",
    "LOSS_FUNCTION = 'mean_squared_error'\n",
    "OPTIMIZER = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# --- Training Configuration ---\n",
    "EPOCHS = 100 # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "print(\"‚úÖ Configuration parameters set.\")\n",
    "print(f\"Target Stock: {STOCK_TICKER}\")\n",
    "print(f\"Features for Model: {FEATURES_TO_USE}\")\n",
    "print(f\"Optimal Window Size: {WINDOW_SIZE} days\")\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section3\"></a>\n",
    "# ## 3. Data Acquisition & Exploration\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown We use the `yfinance` library to download historical stock data. This section includes robust error handling to ensure a valid ticker is provided and data is successfully fetched.\n",
    "# @markdown\n",
    "# [cite_start]@markdown 1.  **Fetch Data:** `yf.download()` pulls the historical market data[cite: 106].\n",
    "# @markdown 2.  **Input Validation:** A `try-except` block validates the ticker symbol. If no data is returned, it prints a clear error message and stops execution.\n",
    "# [cite_start]@markdown 3.  **Inspect Data:** We display the first few rows and check for missing values, which is a critical preprocessing step[cite: 118, 325].\n",
    "# @markdown 4.  **Visualize:** An interactive candlestick chart helps in understanding the data's trend and volatility.\n",
    "\n",
    "# --- Fetch Data with Error Handling ---\n",
    "data = pd.DataFrame()\n",
    "try:\n",
    "    data = yf.download(STOCK_TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
    "    if data.empty:\n",
    "        raise ValueError(f\"No data found for ticker '{STOCK_TICKER}'. It may be an invalid symbol or delisted.\")\n",
    "    print(f\"‚úÖ Successfully downloaded {len(data)} data points for {STOCK_TICKER}.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching data: {e}\")\n",
    "\n",
    "# --- Proceed only if data was downloaded successfully ---\n",
    "if not data.empty:\n",
    "    # --- Inspect Data ---\n",
    "    print(\"\\n--- First 5 Rows of Data ---\")\n",
    "    display(data.head())\n",
    "\n",
    "    print(\"\\n--- Data Info & Missing Values ---\")\n",
    "    # [cite_start]Handling missing values is a critical preprocessing step[cite: 118, 326].\n",
    "    if data.isnull().sum().any():\n",
    "        print(\"Missing values found. Applying dropna().\")\n",
    "        data.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "\n",
    "\n",
    "    # --- Visualize Historical Data ---\n",
    "    fig_explore = go.Figure(data=[go.Candlestick(x=data.index,\n",
    "                                               open=data['Open'],\n",
    "                                               high=data['High'],\n",
    "                                               low=data['Low'],\n",
    "                                               close=data['Close'],\n",
    "                                               name='Candlestick')])\n",
    "\n",
    "    fig_explore.update_layout(\n",
    "        title=f'Historical Price Data for {STOCK_TICKER}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Stock Price (USD)',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    fig_explore.show()\n",
    "else:\n",
    "    print(\"\\nStopping execution due to data fetching failure.\")\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section4\"></a>\n",
    "# ## 4. Feature Engineering (Technical Indicators)\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown We derive new features (technical indicators) from the raw price and volume data. [cite_start]The research highlights that these indicators provide the model with crucial insights into market dynamics, momentum, and volatility, thereby enhancing predictive accuracy[cite: 148, 305].\n",
    "# @markdown\n",
    "# [cite_start]@markdown We implement functions for each indicator mentioned in the source documents, with their formulas included as comments [cite: 306-320]. [cite_start]The critical feature for our best-performing model is **On-Balance Volume (OBV)**[cite: 221].\n",
    "\n",
    "if not data.empty:\n",
    "    # --- Indicator Implementation ---\n",
    "    def calculate_sma(data, window):\n",
    "        # [cite_start]Formula: SMA = (P1 + P2 + ... + Pn) / n [cite: 307]\n",
    "        return data['Close'].rolling(window=window).mean()\n",
    "\n",
    "    def calculate_rsi(data, window=14):\n",
    "        # [cite_start]Formula: RSI = 100 - (100 / (1 + RS)) where RS = Avg Gain / Avg Loss [cite: 310]\n",
    "        delta = data['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "        # [cite_start]Formula: MACD = EMA_short - EMA_long [cite: 312]\n",
    "        short_ema = data['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "        long_ema = data['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "        macd = short_ema - long_ema\n",
    "        signal_line = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "        return macd, signal_line\n",
    "\n",
    "    def calculate_obv(data):\n",
    "        # [cite_start]Formula: If Close > Prev_Close, OBV = Prev_OBV + Volume [cite: 316]\n",
    "        # [cite_start]If Close < Prev_Close, OBV = Prev_OBV - Volume [cite: 317]\n",
    "        obv = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()\n",
    "        return obv\n",
    "\n",
    "    # --- Apply Indicators to DataFrame ---\n",
    "    data['SMA_15'] = calculate_sma(data, 15)\n",
    "    data['SMA_45'] = calculate_sma(data, 45)\n",
    "    data['RSI'] = calculate_rsi(data)\n",
    "    data['MACD'], data['Signal_Line'] = calculate_macd(data)\n",
    "    data['OBV'] = calculate_obv(data)\n",
    "\n",
    "    # Drop initial NaN values created by rolling windows\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    print(\"‚úÖ Technical indicators calculated and added to the dataframe.\")\n",
    "    print(\"--- Data with New Features ---\")\n",
    "    display(data.head())\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section5\"></a>\n",
    "# ## 5. Data Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# [cite_start]@markdown This is a critical phase to prepare the data for our LSTM model, following the pipeline described in the research[cite: 323].\n",
    "# @markdown\n",
    "# [cite_start]@markdown 1.  **Feature Selection:** We select the subset of features specified in our configuration (`Close` and `OBV` by default)[cite: 228].\n",
    "# @markdown 2.  **Normalization:** We use `MinMaxScaler` to scale all selected features to a range of [0, 1]. [cite_start]This is vital for LSTM models as it helps with efficient optimization[cite: 152, 334]. [cite_start]We must save the `scaler` objects to reverse the transformation on our predictions later[cite: 192].\n",
    "# @markdown 3.  **Sequence Generation (Sliding Window):** LSTMs require data in a sequential format. [cite_start]A \"sliding window\" of `WINDOW_SIZE` (60) days of feature data is used as input (`X`) to predict the closing price of the next day (`y`)[cite: 122, 338].\n",
    "# [cite_start]@markdown 4.  **Data Splitting:** The data is split chronologically into training and testing sets to prevent data leakage[cite: 126]. [cite_start]The 85/15 split ratio is used as per Shobayo et al.[cite: 517].\n",
    "\n",
    "if not data.empty:\n",
    "    # --- 1. Feature Selection ---\n",
    "    features = data[FEATURES_TO_USE]\n",
    "    print(f\"Selected features for model: {features.columns.to_list()}\")\n",
    "\n",
    "    # --- 2. Normalization ---\n",
    "    # Separate scalers for features and target for robustness\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_features = feature_scaler.fit_transform(features)\n",
    "    scaled_target = target_scaler.fit_transform(data[['Close']])\n",
    "\n",
    "    print(\"\\n--- Data after Min-Max Scaling ---\")\n",
    "    print(\"Shape of scaled data:\", scaled_features.shape)\n",
    "\n",
    "    # --- 3. Sequence Generation ---\n",
    "    def create_sequences(data, window_size):\n",
    "        X, y = [], []\n",
    "        for i in range(window_size, len(data)):\n",
    "            X.append(data[i-window_size:i, :])\n",
    "            y.append(data[i, 0]) # Target is the first column ('Close')\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Use scaled_features for X and scaled_target for y for alignment\n",
    "    X, y_unsplit = create_sequences(scaled_features, WINDOW_SIZE)\n",
    "    # The y created must align with the target scaler, let's create it from the scaled_target\n",
    "    y = scaled_target[WINDOW_SIZE:].flatten()\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Sequences Created (Window Size: {WINDOW_SIZE}) ---\")\n",
    "    print(f\"Shape of X (input sequences): {X.shape}\")\n",
    "    print(f\"Shape of y (target values): {y.shape}\")\n",
    "\n",
    "    # --- 4. Data Splitting ---\n",
    "    split_index = int(len(X) * TRAIN_TEST_SPLIT_RATIO)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    print(f\"\\n--- Data Split (Train: {TRAIN_TEST_SPLIT_RATIO*100:.0f}%, Test: {(1-TRAIN_TEST_SPLIT_RATIO)*100:.0f}%) ---\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(\"‚úÖ Data preprocessing complete.\")\n",
    "\n",
    "    # --- Memory Management ---\n",
    "    del features, scaled_features, scaled_target, y_unsplit\n",
    "    gc.collect()\n",
    "    print(\"\\nüßπ Unused dataframes cleared from memory.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section6\"></a>\n",
    "# ## 6. LSTM Model Implementation\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown We define the architecture of our LSTM model using TensorFlow's Keras API. [cite_start]The structure is based on the description of the best-performing model from the research, consisting of two stacked LSTM layers, a dropout layer for regularization, and a dense output layer[cite: 256].\n",
    "# @markdown\n",
    "# @markdown 1.  **`Sequential` Model:** A linear stack of layers.\n",
    "# @markdown 2.  **First `LSTM` Layer:** Receives the input sequences. [cite_start]`return_sequences=True` is crucial for stacking another LSTM layer on top[cite: 53, 240].\n",
    "# @markdown 3.  **Second `LSTM` Layer:** Learns higher-level temporal representations.\n",
    "# [cite_start]@markdown 4.  **`Dropout` Layer:** A powerful technique to prevent overfitting by randomly setting a fraction of input units to 0 during training[cite: 54, 241, 439]. [cite_start]The rate of 0.266 is used from the optimized model[cite: 303].\n",
    "# [cite_start]@markdown 5.  **`Dense` Output Layer:** A single neuron to output the predicted (normalized) stock price[cite: 56, 244].\n",
    "# [cite_start]@markdown 6.  **`compile()`:** We configure the model for training with the Adam optimizer and Mean Squared Error loss function, as recommended[cite: 57, 58, 292, 293].\n",
    "\n",
    "if not data.empty:\n",
    "    # --- Define the Model Architecture ---\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM Layer\n",
    "    model.add(LSTM(units=LSTM_UNITS,\n",
    "                   return_sequences=True,\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "    # Second LSTM Layer\n",
    "    model.add(LSTM(units=LSTM_UNITS,\n",
    "                   return_sequences=False))\n",
    "\n",
    "    # Dropout Layer for regularization\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "    # Dense Output Layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # --- Compile the Model ---\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION)\n",
    "\n",
    "    print(\"‚úÖ LSTM Model built successfully.\")\n",
    "    model.summary()\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section7\"></a>\n",
    "# ## 7. Model Training\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown Now we train our compiled LSTM model on the preprocessed training data.\n",
    "# @markdown\n",
    "# @markdown 1.  **Callbacks:** We define two helpful callbacks:\n",
    "# [cite_start]@markdown     - **`EarlyStopping`:** This monitors the validation loss and stops the training process if it doesn't improve for a set number of epochs (`patience`)[cite: 61, 268, 440]. This is a critical best practice to prevent overfitting and save computation time.\n",
    "# @markdown     - **`ModelCheckpoint`:** This saves the best version of the model to a file during training.\n",
    "# @markdown 2.  **`model.fit()`:** This is the main training function. [cite_start]We pass the training data, epochs, batch size, and a validation split to monitor performance on unseen data during training[cite: 131].\n",
    "# @markdown 3.  **Visualize Training History:** After training, we plot the training vs. validation loss. [cite_start]This plot is essential for diagnosing issues like overfitting or underfitting[cite: 137, 162, 164].\n",
    "\n",
    "if not data.empty:\n",
    "    # --- Define Callbacks ---\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               patience=EARLY_STOPPING_PATIENCE,\n",
    "                               restore_best_weights=True,\n",
    "                               verbose=1)\n",
    "\n",
    "    # Mount Google Drive to save the model\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        model_path = f'/content/drive/MyDrive/{STOCK_TICKER}_best_model.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath=model_path,\n",
    "                                     monitor='val_loss',\n",
    "                                     save_best_only=True,\n",
    "                                     verbose=1)\n",
    "        print(\"‚úÖ Google Drive mounted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mounting Google Drive: {e}. Model will not be saved.\")\n",
    "        model_path = None\n",
    "        checkpoint = [] # No checkpoint if drive fails\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop] + ([checkpoint] if model_path else []),\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "\n",
    "    # --- Visualize Training History ---\n",
    "    fig_history = go.Figure()\n",
    "    fig_history.add_trace(go.Scatter(y=history.history['loss'], name='Training Loss'))\n",
    "    fig_history.add_trace(go.Scatter(y=history.history['val_loss'], name='Validation Loss'))\n",
    "    fig_history.update_layout(title='Model Training and Validation Loss Over Epochs',\n",
    "                              xaxis_title='Epochs',\n",
    "                              yaxis_title='Loss (MSE)',\n",
    "                              template='plotly_dark')\n",
    "    fig_history.show()\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section8\"></a>\n",
    "# ## 8. Prediction & Performance Evaluation\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown With our model trained, we evaluate its performance on the unseen test dataset to understand how well it generalizes.\n",
    "# @markdown\n",
    "# [cite_start]@markdown 1.  **Make Predictions:** We use `model.predict()` on `X_test` to get the forecasts[cite: 134].\n",
    "# [cite_start]@markdown 2.  **Inverse Transform:** We convert the normalized predictions and actual values back to their original price scale for interpretation[cite: 135, 192].\n",
    "# [cite_start]@markdown 3.  **Calculate Metrics:** We compute a comprehensive suite of evaluation metrics as recommended by the research, including RMSE, MAE, MAPE, and R-squared [cite: 41, 43, 44, 45, 468-484].\n",
    "# @markdown 4.  **Baseline Comparison:** We compare the LSTM model's performance against a simple \"Naive Forecast\" baseline (predicting today's price is the same as yesterday's). This helps quantify the value added by our complex model.\n",
    "\n",
    "if not data.empty:\n",
    "    # --- 1. Make Predictions ---\n",
    "    predicted_prices_scaled = model.predict(X_test)\n",
    "\n",
    "    # --- 2. Inverse Transform ---\n",
    "    predicted_prices = target_scaler.inverse_transform(predicted_prices_scaled)\n",
    "    actual_prices = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # --- 3. Calculate Performance Metrics ---\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return rmse, mae, mape, r2\n",
    "\n",
    "    lstm_rmse, lstm_mae, lstm_mape, lstm_r2 = calculate_metrics(actual_prices, predicted_prices)\n",
    "\n",
    "\n",
    "    # <a id=\"subsection8_1\"></a>\n",
    "    # ### 8.1 Baseline Model Comparison\n",
    "\n",
    "    # --- Naive Forecast Baseline ---\n",
    "    # The naive forecast predicts the price of day t to be the price of day t-1.\n",
    "    # We need the 'Close' price from the original dataframe corresponding to the test set.\n",
    "    test_data_start_index = split_index + WINDOW_SIZE\n",
    "    # The actual prices for the test set start at test_data_start_index\n",
    "    # The naive prediction for the first test day is the close price of the day before it.\n",
    "    naive_predictions = data['Close'].iloc[test_data_start_index - 1 : -1].values\n",
    "\n",
    "    # Ensure the lengths match\n",
    "    naive_actuals = actual_prices[:len(naive_predictions)]\n",
    "\n",
    "    naive_rmse, naive_mae, naive_mape, naive_r2 = calculate_metrics(naive_actuals, naive_predictions)\n",
    "\n",
    "    # --- Display Metrics in a Comparison Table ---\n",
    "    metrics_data = {\n",
    "        'Metric': ['RMSE (USD)', 'MAE (USD)', 'MAPE (%)', 'R-squared (R¬≤)'],\n",
    "        'LSTM Model': [f\"{lstm_rmse:.4f}\", f\"{lstm_mae:.4f}\", f\"{lstm_mape:.4f}\", f\"{lstm_r2:.4f}\"],\n",
    "        'Naive Baseline': [f\"{naive_rmse:.4f}\", f\"{naive_mae:.4f}\", f\"{naive_mape:.4f}\", f\"{naive_r2:.4f}\"],\n",
    "        'Benchmark (Shobayo)': ['0.018522 (scaled)', 'N/A', '1.33', '0.993']\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    print(\"\\n--- Model Performance Comparison ---\")\n",
    "    display(metrics_df.style.hide_index())\n",
    "\n",
    "    # <a id=\"subsection8_2\"></a>\n",
    "    # ### 8.2 Visualizing Results\n",
    "\n",
    "    # --- Visualize Predictions vs Actual Prices ---\n",
    "    test_dates = data.index[split_index + WINDOW_SIZE:]\n",
    "\n",
    "    fig_results = go.Figure()\n",
    "    fig_results.add_trace(go.Scatter(x=test_dates, y=actual_prices.flatten(),\n",
    "                                     mode='lines', name='Actual Prices',\n",
    "                                     line=dict(color='cyan')))\n",
    "    fig_results.add_trace(go.Scatter(x=test_dates, y=predicted_prices.flatten(),\n",
    "                                     mode='lines', name='LSTM Predicted Prices',\n",
    "                                     line=dict(color='magenta')))\n",
    "\n",
    "    fig_results.update_layout(\n",
    "        title=f'LSTM Prediction vs. Actual Prices for {STOCK_TICKER}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Stock Price (USD)',\n",
    "        template='plotly_dark',\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "    )\n",
    "    fig_results.show()\n",
    "\n",
    "    # <a id=\"subsection8_3\"></a>\n",
    "    # ### 8.3 Exporting Predictions\n",
    "\n",
    "    # --- Create a DataFrame with predictions and offer for download ---\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Date': test_dates,\n",
    "        'Actual_Price': actual_prices.flatten(),\n",
    "        'Predicted_Price': predicted_prices.flatten()\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = f'{STOCK_TICKER}_predictions.csv'\n",
    "    predictions_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Predictions saved to '{csv_filename}'.\")\n",
    "    # Offer file for download in Colab\n",
    "    try:\n",
    "      files.download(csv_filename)\n",
    "      print(f\"Download started for {csv_filename}.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Could not automatically download file. Please find it in the Colab file explorer to the left.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section9\"></a>\n",
    "# ## 9. Saving and Loading the Model\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown After training, we save the model for future use without retraining.\n",
    "# @markdown\n",
    "# @markdown 1.  **Saving:** The `ModelCheckpoint` callback has already saved the best model to Google Drive.\n",
    "# @markdown 2.  **Loading:** We show how to load this saved model using `tf.keras.models.load_model()`.\n",
    "# @markdown 3.  **Verification:** We use the loaded model to make a prediction and compare it to the original model's prediction to ensure they are identical.\n",
    "\n",
    "if not data.empty and model_path and os.path.exists(model_path):\n",
    "    print(f\"The best model has been saved to: {model_path}\")\n",
    "\n",
    "    # --- Load the Saved Model ---\n",
    "    try:\n",
    "        loaded_model = tf.keras.models.load_model(model_path)\n",
    "        print(\"\\n‚úÖ Successfully loaded the saved model from Google Drive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        loaded_model = None\n",
    "\n",
    "    # --- Verification ---\n",
    "    if loaded_model:\n",
    "        original_prediction = model.predict(X_test[:1], verbose=0)\n",
    "        loaded_prediction = loaded_model.predict(X_test[:1], verbose=0)\n",
    "\n",
    "        if np.isclose(original_prediction, loaded_prediction):\n",
    "            print(\"‚úÖ Verification successful: Loaded model prediction matches original model.\")\n",
    "        else:\n",
    "            print(\"‚ùå Verification failed: Predictions do not match.\")\n",
    "else:\n",
    "    print(\"Skipping model loading as the model file was not saved or found.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section10\"></a>\n",
    "# ## 10. Live Prediction on New Data\n",
    "\"\"\"\n",
    "\n",
    "# @markdown ### What's happening here?\n",
    "# @markdown This function simulates a real-time prediction for the next trading day. It fetches the latest available data, processes it using the same pipeline (feature calculation, scaling, sequencing), and feeds it into the trained model to generate a forecast.\n",
    "\n",
    "def predict_next_day(ticker, model_to_use, feature_scaler_to_use, target_scaler_to_use):\n",
    "    \"\"\"\n",
    "    Predicts the next day's closing price for a given stock.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Live Prediction for {ticker} ---\")\n",
    "    try:\n",
    "        # 1. Fetch latest data (enough to calculate indicators and form a window)\n",
    "        latest_data = yf.download(ticker, period='6mo', progress=False)\n",
    "        if latest_data.empty:\n",
    "          print(\"‚ùå Could not fetch latest data.\")\n",
    "          return None\n",
    "\n",
    "        # 2. Calculate necessary features\n",
    "        latest_data['OBV'] = calculate_obv(latest_data)\n",
    "        latest_data.dropna(inplace=True)\n",
    "\n",
    "        # 3. Get the last WINDOW_SIZE days\n",
    "        last_window_data = latest_data[FEATURES_TO_USE].tail(WINDOW_SIZE)\n",
    "        if len(last_window_data) < WINDOW_SIZE:\n",
    "            print(\"‚ùå Not enough recent data to form a full prediction window.\")\n",
    "            return None\n",
    "\n",
    "        # 4. Scale the data using the *same* scaler from training\n",
    "        scaled_window = feature_scaler_to_use.transform(last_window_data)\n",
    "\n",
    "        # 5. Reshape for the model\n",
    "        X_pred = np.reshape(scaled_window, (1, WINDOW_SIZE, len(FEATURES_TO_USE)))\n",
    "\n",
    "        # 6. Predict\n",
    "        predicted_scaled_price = model_to_use.predict(X_pred, verbose=0)\n",
    "\n",
    "        # 7. Inverse transform to get the actual price\n",
    "        predicted_price = target_scaler_to_use.inverse_transform(predicted_scaled_price)\n",
    "\n",
    "        return predicted_price[0][0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during live prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "if 'loaded_model' in locals() and loaded_model is not None:\n",
    "    next_day_prediction = predict_next_day(STOCK_TICKER, loaded_model, feature_scaler, target_scaler)\n",
    "    if next_day_prediction:\n",
    "        print(f\"\\n BOLD_TEXT‚úÖ Predicted closing price for the next trading day for {STOCK_TICKER}: ${next_day_prediction:.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "# <a id=\"section11\"></a>\n",
    "# ## 11. Advanced Topics & Future Work\n",
    "\"\"\"\n",
    "\n",
    "# @markdown This section explores advanced concepts and future development directions discussed in the source documents.\n",
    "# @markdown\n",
    "# @markdown ### Sentiment Analysis Integration\n",
    "# [cite_start]@markdown The research by Chaudhary (2022) showed that integrating sentiment scores from news can improve accuracy by 8-12%[cite: 213]. A full implementation would require:\n",
    "# [cite_start]@markdown 1.  **Data Acquisition:** Using a news API (e.g., Bloomberg, Reuters) to fetch real-time text data[cite: 187].\n",
    "# [cite_start]@markdown 2.  **Temporal Alignment:** Aggregating sentiment scores (using a tool like VADER) for each trading day[cite: 115, 116].\n",
    "# [cite_start]@markdown 3.  **Integration:** Adding the normalized sentiment score as an additional feature to the model[cite: 132].\n",
    "# @markdown\n",
    "# @markdown ### API Deployment for Real-time Inference\n",
    "# [cite_start]@markdown For production use, the model should be deployed as a RESTful API[cite: 431]. This would allow trading platforms and dashboards to request predictions programmatically. A simple implementation could use a web framework like Flask or FastAPI.\n",
    "# @markdown\n",
    "# @markdown ### Robustness: Fallback Data Sources\n",
    "# @markdown For a mission-critical system, relying on a single data source is risky. A robust implementation would include fallback logic.\n",
    "# @markdown ```python\n",
    "# @markdown def get_data_robust(ticker):\n",
    "# @markdown     try:\n",
    "# @markdown         # Try primary source\n",
    "# @markdown         data = yf.download(ticker)\n",
    "# @markdown         if data.empty: raise ValueError(\"No data from yfinance\")\n",
    "# @markdown         return data\n",
    "# @markdown     except Exception as e:\n",
    "# @markdown         print(f\"Primary source failed: {e}. Trying fallback...\")\n",
    "# @markdown         # Try secondary source (e.g., Alpha Vantage, IEX Cloud)\n",
    "# @markdown         # data = alpha_vantage.get_daily(...)\n",
    "# @markdown         return data # or None if all fail\n",
    "# @markdown ```\n",
    "# @markdown\n",
    "# @markdown ### Memory Management for Large Datasets\n",
    "# @markdown While stock data is typically manageable, for extremely large datasets (e.g., tick-level data over decades), memory can be an issue.\n",
    "# @markdown - **Data Types:** Use memory-efficient data types in pandas (e.g., `float32` instead of `float64`).\n",
    "# @markdown - **Garbage Collection:** We use `del` and `gc.collect()` to manually free up memory after variables are no longer needed.\n",
    "# @markdown - **Generators/tf.data:** For datasets that don't fit in memory, use Python generators or the `tf.data` API to feed data to the model in batches without loading everything at once."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7n+Wwt5dzC0XQPuLRIhA+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
