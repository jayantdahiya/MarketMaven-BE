{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Professional Stock Market Prediction with LSTM\n",
    "\n",
    "This notebook provides a complete, production-ready guide to implementing a sophisticated stock prediction model using Long Short-Term Memory (LSTM) networks. The methodology is synthesized from common findings in financial machine learning research, focusing on high-performing architectures and techniques to create a practical and robust tool.\n",
    "\n",
    "**Core Methodology:** The implementation is based on a common and effective LSTM architecture. This model uses a 60-day time step (window) and incorporates On-Balance Volume (OBV) as a key feature alongside the closing price. Such architectures have been shown in various studies to be effective for time-series forecasting.\n",
    "\n",
    "### Key Features of this Notebook:\n",
    "* **Interactive & Customizable:** An interactive widget allows you to easily change the stock ticker and other key parameters.\n",
    "* **End-to-End Pipeline:** Covers the entire workflow from data acquisition and feature engineering to model training, evaluation, and visualization.\n",
    "* **Automated Evaluation:** Automatically evaluates the LSTM model against a naive baseline to provide a clear performance benchmark.\n",
    "* **Robust & Reliable:** Includes input validation, error handling, and the saving of both the model and the necessary data scalers for reliable future predictions.\n",
    "* **Practical Outputs:** Provides functionality to save the trained model, load it for future use, and export predictions to a CSV file.\n",
    "\n",
    "### Theoretical Backing (Adaptive Markets Hypothesis)\n",
    "While the traditional Efficient Market Hypothesis (EMH) suggests that market prices are unpredictable, this notebook's approach is better supported by the **Adaptive Markets Hypothesis (AMH)**. The AMH posits that markets are not perfectly efficient and that temporary inefficiencies arise, which advanced machine learning models like LSTM can learn to exploit for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1.  [Setup & Dependencies](#section1)\n",
    "2.  [Configuration & Parameters](#section2)\n",
    "3.  [Data Acquisition & Exploration](#section3)\n",
    "4.  [Feature Engineering (Technical Indicators)](#section4)\n",
    "5.  [Data Preprocessing](#section5)\n",
    "6.  [LSTM Model Implementation](#section6)\n",
    "7.  [Model Training](#section7)\n",
    "8.  [Prediction & Performance Evaluation](#section8)\n",
    "    * [8.1 Baseline Model Comparison](#subsection8_1)\n",
    "    * [8.2 Visualizing Results](#subsection8_2)\n",
    "    * [8.3 Exporting Predictions](#subsection8_3)\n",
    "9.  [Saving and Loading the Model & Scalers](#section9)\n",
    "10. [Live Prediction on New Data](#section10)\n",
    "11. [Advanced Topics & Future Work](#section11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section1\"></a>\n",
    "## 1. Setup & Dependencies\n",
    "\n",
    "### What's happening here?\n",
    "We are installing and importing all the necessary Python libraries for our project.\n",
    "- `yfinance`: To fetch historical stock market data from Yahoo Finance.\n",
    "- `pandas` & `numpy`: For data manipulation and numerical operations.\n",
    "- `scikit-learn`: For data preprocessing (scaling with `MinMaxScaler`, saving scalers with `joblib`) and evaluation metrics.\n",
    "- `tensorflow`: The deep learning framework to build and train our LSTM model.\n",
    "- `plotly`: To create interactive and professional-looking visualizations.\n",
    "\n",
    "The code block also checks for GPU availability to accelerate model training, a key performance optimization technique.\n",
    "\n",
    "**Run this cell to install and import dependencies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install yfinance pandas numpy scikit-learn tensorflow plotly joblib kaleido -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import joblib # For saving and loading scalers\n",
    "\n",
    "# The following imports are for Google Colab. If running elsewhere, you might need to adapt this.\n",
    "try:\n",
    "    from google.colab import drive, files\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPU Check ---\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('‚ö†Ô∏è GPU device not found. Training will be on CPU.')\n",
    "else:\n",
    "  print(f'‚úÖ Found GPU at: {device_name}. Training will be GPU-accelerated.')\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "print(\"‚úÖ Dependencies installed and imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section2\"></a>\n",
    "## 2. Configuration & Parameters\n",
    "\n",
    "### What's happening here?\n",
    "This interactive form centralizes all key parameters, making the notebook highly reusable and easy to customize. The default values are based on configurations commonly found to perform well in financial forecasting research.\n",
    "\n",
    "- **Model:** A stacked LSTM architecture using a 60-Day Time Step with OBV.\n",
    "- **Stock:** Enter any valid Yahoo Finance ticker symbol. `AAPL` is the default.\n",
    "- **Date Range:** A long period provides more data for robust training. The end date dynamically defaults to the current day.\n",
    "- **Window Size:** Set to 60, a common choice that has been found to be empirically effective in multiple studies.\n",
    "- **Features:** The list includes `Close` and `OBV`. Research suggests that combining price with volume-based indicators can improve model performance.\n",
    "- **Hyperparameters:** These values (units, dropout, learning rate) are example settings that serve as a strong starting point for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Configuration ---\n",
    "STOCK_TICKER = \"AAPL\"  # @param {type:\"string\"}\n",
    "START_DATE = \"2010-01-01\"  # @param {type:\"date\"}\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d') # @param {type:\"date\"}\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.85  # @param {type:\"slider\", min:0.7, max:0.9, step:0.05}\n",
    "\n",
    "# --- Model & Feature Configuration ---\n",
    "WINDOW_SIZE = 60  # @param {type:\"integer\"}\n",
    "# The model will use 'Close' and 'OBV'. You can experiment by adding more features here.\n",
    "# e.g., ['Close', 'OBV', 'RSI', 'MACD']\n",
    "FEATURES_TO_USE = ['Close', 'OBV']\n",
    "\n",
    "# --- LSTM Hyperparameters (example values) ---\n",
    "LSTM_UNITS = 104  # @param {type:\"integer\"}\n",
    "DROPOUT_RATE = 0.266  # @param {type:\"number\"}\n",
    "LEARNING_RATE = 0.00369  # @param {type:\"number\"}\n",
    "LOSS_FUNCTION = 'mean_squared_error'\n",
    "OPTIMIZER = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# --- Training Configuration ---\n",
    "EPOCHS = 100  # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 32  # @param {type:\"integer\"}\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "# --- File Paths for Saving Artifacts ---\n",
    "# We will save the model, and the scalers, to be able to make predictions later.\n",
    "SAVE_DIR = \"model_artifacts\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, f'{STOCK_TICKER}_best_model.h5')\n",
    "FEATURE_SCALER_PATH = os.path.join(SAVE_DIR, f'{STOCK_TICKER}_feature_scaler.joblib')\n",
    "TARGET_SCALER_PATH = os.path.join(SAVE_DIR, f'{STOCK_TICKER}_target_scaler.joblib')\n",
    "\n",
    "print(\"‚úÖ Configuration parameters set.\")\n",
    "print(f\"Target Stock: {STOCK_TICKER}\")\n",
    "print(f\"Features for Model: {FEATURES_TO_USE}\")\n",
    "print(f\"Window Size: {WINDOW_SIZE} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section3\"></a>\n",
    "## 3. Data Acquisition & Exploration\n",
    "\n",
    "### What's happening here?\n",
    "We use the `yfinance` library to download historical stock data. This section includes robust error handling to ensure a valid ticker is provided and data is successfully fetched.\n",
    "\n",
    "1.  **Fetch Data:** `yf.download()` pulls the historical market data.\n",
    "2.  **Input Validation:** A `try-except` block validates the ticker symbol. If no data is returned, it prints a clear error message and stops execution.\n",
    "3.  **Inspect Data:** We display the first few rows and check for missing values, which is a critical preprocessing step.\n",
    "4.  **Visualize:** An interactive candlestick chart helps in understanding the data's trend and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fetch Data with Error Handling ---\n",
    "data = pd.DataFrame()\n",
    "try:\n",
    "    data = yf.download(STOCK_TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
    "    if data.empty:\n",
    "        raise ValueError(f\"No data found for ticker '{STOCK_TICKER}'. It may be an invalid symbol or delisted.\")\n",
    "    print(f\"‚úÖ Successfully downloaded {len(data)} data points for {STOCK_TICKER}.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Proceed only if data was downloaded successfully ---\n",
    "if not data.empty:\n",
    "    # --- Inspect Data ---\n",
    "    print(\"\\n--- First 5 Rows of Data ---\")\n",
    "    display(data.head())\n",
    "\n",
    "    print(\"\\n--- Data Info & Missing Values ---\")\n",
    "    # Handling missing values is a critical preprocessing step.\n",
    "    if data.isnull().sum().any():\n",
    "        print(\"Missing values found. Applying forward-fill.\")\n",
    "        data.ffill(inplace=True)\n",
    "        data.dropna(inplace=True) # Drop any remaining NaNs at the beginning\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "else:\n",
    "    print(\"\\nStopping execution due to data fetching failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Visualize Historical Data ---\n",
    "    fig_explore = go.Figure(data=[go.Candlestick(x=data.index,\n",
    "                                               open=data['Open'],\n",
    "                                               high=data['High'],\n",
    "                                               low=data['Low'],\n",
    "                                               close=data['Close'],\n",
    "                                               name='Candlestick')])\n",
    "\n",
    "    fig_explore.update_layout(\n",
    "        title=f'Historical Price Data for {STOCK_TICKER}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Stock Price (USD)',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    fig_explore.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section4\"></a>\n",
    "## 4. Feature Engineering (Technical Indicators)\n",
    "\n",
    "### What's happening here?\n",
    "We derive new features (technical indicators) from the raw price and volume data. Research highlights that these indicators can provide the model with crucial insights into market dynamics, momentum, and volatility, thereby enhancing predictive accuracy.\n",
    "\n",
    "We implement functions for each indicator, with their formulas included as comments. The critical feature for our model is **On-Balance Volume (OBV)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Indicator Implementation ---\n",
    "    def calculate_sma(data, window):\n",
    "        # Formula: SMA = (P1 + P2 + ... + Pn) / n\n",
    "        return data['Close'].rolling(window=window).mean()\n",
    "\n",
    "    def calculate_rsi(data, window=14):\n",
    "        # Formula: RSI = 100 - (100 / (1 + RS)) where RS = Avg Gain / Avg Loss\n",
    "        delta = data['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        # Add a small epsilon to avoid division by zero\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "        # Formula: MACD = EMA_short - EMA_long\n",
    "        short_ema = data['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "        long_ema = data['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "        macd = short_ema - long_ema\n",
    "        signal_line = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "        return macd, signal_line\n",
    "\n",
    "    def calculate_obv(data):\n",
    "        # Formula: If Close > Prev_Close, OBV = Prev_OBV + Volume\n",
    "        # If Close < Prev_Close, OBV = Prev_OBV - Volume\n",
    "        obv = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()\n",
    "        return obv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Apply Indicators to DataFrame ---\n",
    "    data['SMA_15'] = calculate_sma(data, 15)\n",
    "    data['SMA_45'] = calculate_sma(data, 45)\n",
    "    data['RSI'] = calculate_rsi(data)\n",
    "    data['MACD'], data['Signal_Line'] = calculate_macd(data)\n",
    "    data['OBV'] = calculate_obv(data)\n",
    "\n",
    "    # Drop initial NaN values created by rolling windows\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    print(\"‚úÖ Technical indicators calculated and added to the dataframe.\")\n",
    "    print(\"--- Data with New Features ---\")\n",
    "    display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section5\"></a>\n",
    "## 5. Data Preprocessing\n",
    "\n",
    "### What's happening here?\n",
    "This is a critical phase to prepare the data for our LSTM model.\n",
    "\n",
    "1.  **Feature Selection:** We select the subset of features specified in our configuration (`Close` and `OBV` by default).\n",
    "2.  **Normalization:** We use `MinMaxScaler` to scale all selected features to a range of [0, 1]. This is vital for LSTM models as it helps with efficient optimization. We use two separate scalers: one for the input features and one for the target variable (`Close`). This is a best practice that prevents data leakage from other features into the target's scaling, making the inverse transform of our prediction more robust.\n",
    "3.  **Sequence Generation (Sliding Window):** LSTMs require data in a sequential format. A \"sliding window\" of `WINDOW_SIZE` (60) days of feature data is used as input (`X`) to predict the closing price of the next day (`y`).\n",
    "4.  **Data Splitting:** The data is split chronologically into training and testing sets to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- 1. Feature Selection ---\n",
    "    features = data[FEATURES_TO_USE]\n",
    "    print(f\"Selected features for model: {features.columns.to_list()}\")\n",
    "\n",
    "    # --- 2. Normalization ---\n",
    "    # We use two separate scalers. This is best practice.\n",
    "    # feature_scaler is for all input features (e.g., Close, OBV).\n",
    "    # target_scaler is only for the target ('Close'), making inverse transform straightforward.\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_features = feature_scaler.fit_transform(features)\n",
    "    scaled_target = target_scaler.fit_transform(data[['Close']])\n",
    "\n",
    "    print(\"\\n--- Data after Min-Max Scaling ---\")\n",
    "    print(\"Shape of scaled data:\", scaled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- 3. Sequence Generation ---\n",
    "    def create_sequences(data_features, data_target, window_size):\n",
    "        X, y = [], []\n",
    "        for i in range(window_size, len(data_features)):\n",
    "            X.append(data_features[i-window_size:i, :])\n",
    "            y.append(data_target[i, 0]) # Target is the scaled 'Close' price\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X, y = create_sequences(scaled_features, scaled_target, WINDOW_SIZE)\n",
    "\n",
    "    print(f\"\\n--- Sequences Created (Window Size: {WINDOW_SIZE}) ---\")\n",
    "    print(f\"Shape of X (input sequences): {X.shape}\")\n",
    "    print(f\"Shape of y (target values): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- 4. Data Splitting ---\n",
    "    split_index = int(len(X) * TRAIN_TEST_SPLIT_RATIO)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    print(f\"\\n--- Data Split (Train: {TRAIN_TEST_SPLIT_RATIO*100:.0f}%, Test: {(1-TRAIN_TEST_SPLIT_RATIO)*100:.0f}%) ---\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(\"‚úÖ Data preprocessing complete.\")\n",
    "\n",
    "    # --- Memory Management ---\n",
    "    del features, scaled_features, scaled_target\n",
    "    gc.collect()\n",
    "    print(\"\\nüßπ Unused dataframes cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section6\"></a>\n",
    "## 6. LSTM Model Implementation\n",
    "\n",
    "### What's happening here?\n",
    "We define the architecture of our LSTM model using TensorFlow's Keras API. The structure is a stacked LSTM, a common approach for learning complex temporal patterns.\n",
    "\n",
    "1.  **`Sequential` Model:** A linear stack of layers.\n",
    "2.  **First `LSTM` Layer:** Receives the input sequences. `return_sequences=True` is crucial for stacking another LSTM layer on top.\n",
    "3.  **Second `LSTM` Layer:** Learns higher-level temporal representations.\n",
    "4.  **`Dropout` Layer:** A powerful technique to prevent overfitting by randomly setting a fraction of input units to 0 during training.\n",
    "5.  **`Dense` Output Layer:** A single neuron to output the predicted (normalized) stock price.\n",
    "6.  **`compile()`:** We configure the model for training with the Adam optimizer and Mean Squared Error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Define the Model Architecture ---\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM Layer with Dropout\n",
    "    model.add(LSTM(units=LSTM_UNITS,\n",
    "                   return_sequences=True,\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "    # Second LSTM Layer\n",
    "    model.add(LSTM(units=LSTM_UNITS, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "    # Dense Output Layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # --- Compile the Model ---\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION)\n",
    "\n",
    "    print(\"‚úÖ LSTM Model built successfully.\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section7\"></a>\n",
    "## 7. Model Training\n",
    "\n",
    "### What's happening here?\n",
    "Now we train our compiled LSTM model on the preprocessed training data.\n",
    "\n",
    "1.  **Callbacks:** We define two helpful callbacks:\n",
    "    - **`EarlyStopping`:** This monitors the validation loss and stops the training process if it doesn't improve for a set number of epochs (`patience`). This is a critical best practice to prevent overfitting and save computation time.\n",
    "    - **`ModelCheckpoint`:** This saves the best version of the model to a file (`MODEL_PATH`) during training.\n",
    "2.  **`model.fit()`:** This is the main training function. We pass the training data, epochs, batch size, and a validation split to monitor performance on unseen data during training.\n",
    "3.  **Visualize Training History:** After training, we plot the training vs. validation loss. This plot is essential for diagnosing issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Define Callbacks ---\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               patience=EARLY_STOPPING_PATIENCE,\n",
    "                               restore_best_weights=True,\n",
    "                               verbose=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=MODEL_PATH,\n",
    "                                 monitor='val_loss',\n",
    "                                 save_best_only=True,\n",
    "                                 verbose=1)\n",
    "    \n",
    "    # --- Train the Model ---\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1, # Use 10% of training data for validation\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\n--- Model Training Complete ---\")\n",
    "    \n",
    "    # --- Save the scalers ---\n",
    "    # It's crucial to save the scalers used to preprocess the data\n",
    "    joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "    joblib.dump(target_scaler, TARGET_SCALER_PATH)\n",
    "    print(f\"‚úÖ Model and scalers saved to '{SAVE_DIR}/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty and 'history' in locals():\n",
    "    # --- Visualize Training History ---\n",
    "    fig_history = go.Figure()\n",
    "    fig_history.add_trace(go.Scatter(y=history.history['loss'], name='Training Loss'))\n",
    "    fig_history.add_trace(go.Scatter(y=history.history['val_loss'], name='Validation Loss'))\n",
    "    fig_history.update_layout(title='Model Training and Validation Loss Over Epochs',\n",
    "                              xaxis_title='Epochs',\n",
    "                              yaxis_title='Loss (MSE)',\n",
    "                              template='plotly_dark')\n",
    "    fig_history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section8\"></a>\n",
    "## 8. Prediction & Performance Evaluation\n",
    "\n",
    "### What's happening here?\n",
    "With our model trained, we evaluate its performance on the unseen test dataset to understand how well it generalizes.\n",
    "\n",
    "1.  **Load Best Model:** We load the best model saved by `ModelCheckpoint` to ensure we evaluate the top-performing iteration.\n",
    "2.  **Make Predictions:** We use `model.predict()` on `X_test` to get the forecasts.\n",
    "3.  **Inverse Transform:** We convert the normalized predictions and actual values back to their original price scale for interpretation.\n",
    "4.  **Calculate Metrics:** We compute a comprehensive suite of evaluation metrics, including RMSE, MAE, MAPE, and R-squared.\n",
    "5.  **Baseline Comparison:** We compare the LSTM model's performance against a simple \"Naive Forecast\" baseline (predicting today's price is the same as yesterday's). This helps quantify the value added by our complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- 1. Load the best model saved by ModelCheckpoint\n",
    "    # The 'restore_best_weights=True' in EarlyStopping might make this redundant,\n",
    "    # but loading explicitly is a robust practice.\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        print(\"‚úÖ Best model loaded from file for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load model from {MODEL_PATH}. Using the model from the last training epoch. Error: {e}\")\n",
    "\n",
    "    # --- 2. Make Predictions ---\n",
    "    predicted_prices_scaled = model.predict(X_test)\n",
    "\n",
    "    # --- 3. Inverse Transform ---\n",
    "    # Important: Use the 'target_scaler' that was fitted only on the 'Close' price\n",
    "    predicted_prices = target_scaler.inverse_transform(predicted_prices_scaled)\n",
    "    actual_prices = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # --- 4. Calculate Performance Metrics ---\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return rmse, mae, mape, r2\n",
    "\n",
    "    lstm_rmse, lstm_mae, lstm_mape, lstm_r2 = calculate_metrics(actual_prices, predicted_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"subsection8_1\"></a>\n",
    "### 8.1 Baseline Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Naive Forecast Baseline ---\n",
    "    # The naive forecast predicts the price of day 't' to be the price of day 't-1'.\n",
    "    # We get the 'Close' price from the original dataframe that corresponds to the test set.\n",
    "    test_data_start_index = split_index + WINDOW_SIZE\n",
    "    # Naive prediction for the first test day is the close price of the day before it.\n",
    "    naive_predictions = data['Close'].iloc[test_data_start_index - 1 : -1].values\n",
    "\n",
    "    # Ensure the lengths match for comparison\n",
    "    actuals_for_naive = actual_prices[:len(naive_predictions)]\n",
    "\n",
    "    naive_rmse, naive_mae, naive_mape, naive_r2 = calculate_metrics(actuals_for_naive, naive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Display Metrics in a Comparison Table ---\n",
    "    metrics_data = {\n",
    "        'Metric': ['RMSE (USD)', 'MAE (USD)', 'MAPE (%)', 'R-squared (R¬≤)'],\n",
    "        'LSTM Model': [f\"{lstm_rmse:.4f}\", f\"{lstm_mae:.4f}\", f\"{lstm_mape:.4f}\", f\"{lstm_r2:.4f}\"],\n",
    "        'Naive Baseline': [f\"{naive_rmse:.4f}\", f\"{naive_mae:.4f}\", f\"{naive_mape:.4f}\", f\"{naive_r2:.4f}\"]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    print(\"\\n--- Model Performance Comparison ---\")\n",
    "    display(metrics_df.style.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"subsection8_2\"></a>\n",
    "### 8.2 Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Visualize Predictions vs Actual Prices ---\n",
    "    # Get the dates that correspond to the test set\n",
    "    test_dates = data.index[split_index + WINDOW_SIZE:]\n",
    "\n",
    "    fig_results = go.Figure()\n",
    "    fig_results.add_trace(go.Scatter(x=test_dates, y=actual_prices.flatten(),\n",
    "                                     mode='lines', name='Actual Prices',\n",
    "                                     line=dict(color='cyan')))\n",
    "    fig_results.add_trace(go.Scatter(x=test_dates, y=predicted_prices.flatten(),\n",
    "                                     mode='lines', name='LSTM Predicted Prices',\n",
    "                                     line=dict(color='magenta', dash='dash')))\n",
    "\n",
    "    fig_results.update_layout(\n",
    "        title=f'LSTM Prediction vs. Actual Prices for {STOCK_TICKER}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Stock Price (USD)',\n",
    "        template='plotly_dark',\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "    )\n",
    "    fig_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"subsection8_3\"></a>\n",
    "### 8.3 Exporting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # --- Create a DataFrame with predictions and offer for download ---\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Date': test_dates,\n",
    "        'Actual_Price': actual_prices.flatten(),\n",
    "        'Predicted_Price': predicted_prices.flatten()\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f'{STOCK_TICKER}_predictions.csv'\n",
    "    predictions_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Predictions saved to '{csv_filename}'.\")\n",
    "    # Offer file for download if in Colab\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "          files.download(csv_filename)\n",
    "          print(f\"Download started for {csv_filename}.\")\n",
    "        except Exception as e:\n",
    "          print(f\"Could not automatically download file. Please find it in the Colab file explorer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section9\"></a>\n",
    "## 9. Saving and Loading the Model & Scalers\n",
    "\n",
    "### What's happening here?\n",
    "We have already saved the model and scalers during the training phase. This section demonstrates how to load them back from disk for future use, which is essential for making predictions without retraining.\n",
    "\n",
    "1.  **Loading:** We load the saved model (`.h5` file) and the scaler objects (`.joblib` files).\n",
    "2.  **Verification:** We use the loaded model to make a prediction and compare it to the original model's prediction to ensure they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty and os.path.exists(MODEL_PATH):\n",
    "    print(f\" artifacts have been saved in: {SAVE_DIR}\")\n",
    "\n",
    "    # --- Load the Saved Model and Scalers ---\n",
    "    try:\n",
    "        loaded_model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        loaded_feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "        loaded_target_scaler = joblib.load(TARGET_SCALER_PATH)\n",
    "        print(\"\\n‚úÖ Successfully loaded the saved model and scalers from disk.\")\n",
    "        \n",
    "        # --- Verification ---\n",
    "        original_prediction = model.predict(X_test[:1], verbose=0)\n",
    "        loaded_prediction = loaded_model.predict(X_test[:1], verbose=0)\n",
    "    \n",
    "        if np.allclose(original_prediction, loaded_prediction):\n",
    "            print(\"‚úÖ Verification successful: Loaded model prediction matches original model.\")\n",
    "        else:\n",
    "            print(\"‚ùå Verification failed: Predictions do not match.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading artifacts: {e}\")\n",
    "        loaded_model = None\n",
    "else:\n",
    "    print(\"Skipping model loading as model artifacts were not found.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section10\"></a>\n",
    "## 10. Live Prediction on New Data\n",
    "\n",
    "### What's happening here?\n",
    "This function simulates a real-world prediction for the next trading day. It fetches the latest available data, processes it using the *same pipeline and saved scalers* from training, and feeds it into the loaded model to generate a forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day(ticker, model_path, feature_scaler_path, target_scaler_path):\n",
    "    \"\"\"\n",
    "    Loads a trained model and scalers to predict the next day's closing price.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol.\n",
    "        model_path (str): Path to the saved Keras model file.\n",
    "        feature_scaler_path (str): Path to the saved feature scaler joblib file.\n",
    "        target_scaler_path (str): Path to the saved target scaler joblib file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Live Prediction for {ticker} ---\")\n",
    "    try:\n",
    "        # 1. Load model and scalers\n",
    "        if not all(os.path.exists(p) for p in [model_path, feature_scaler_path, target_scaler_path]):\n",
    "            print(\"‚ùå Missing model or scaler file. Please train the model first.\")\n",
    "            return None\n",
    "        \n",
    "        model_to_use = tf.keras.models.load_model(model_path)\n",
    "        feature_scaler_to_use = joblib.load(feature_scaler_path)\n",
    "        target_scaler_to_use = joblib.load(target_scaler_path)\n",
    "\n",
    "        # 2. Fetch latest data (enough to calculate indicators and form a window)\n",
    "        # Fetch more than needed to ensure indicators can be calculated\n",
    "        latest_data = yf.download(ticker, period='6mo', progress=False)\n",
    "        if len(latest_data) < WINDOW_SIZE:\n",
    "            print(\"‚ùå Not enough recent data to form a prediction window.\")\n",
    "            return None\n",
    "\n",
    "        # 3. Calculate necessary features\n",
    "        latest_data['OBV'] = calculate_obv(latest_data) # Uses the function defined earlier\n",
    "        latest_data.dropna(inplace=True)\n",
    "\n",
    "        # 4. Get the last WINDOW_SIZE days of the required features\n",
    "        last_window_data = latest_data[FEATURES_TO_USE].tail(WINDOW_SIZE)\n",
    "\n",
    "        # 5. Scale the data using the *loaded* scaler from training\n",
    "        scaled_window = feature_scaler_to_use.transform(last_window_data)\n",
    "\n",
    "        # 6. Reshape for the model [1, window_size, num_features]\n",
    "        X_pred = np.reshape(scaled_window, (1, WINDOW_SIZE, len(FEATURES_TO_USE)))\n",
    "\n",
    "        # 7. Predict\n",
    "        predicted_scaled_price = model_to_use.predict(X_pred, verbose=0)\n",
    "\n",
    "        # 8. Inverse transform to get the actual price value\n",
    "        predicted_price = target_scaler_to_use.inverse_transform(predicted_scaled_price)\n",
    "\n",
    "        return predicted_price[0][0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during live prediction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute the live prediction ---\n",
    "if not data.empty:\n",
    "    next_day_prediction = predict_next_day(STOCK_TICKER, MODEL_PATH, FEATURE_SCALER_PATH, TARGET_SCALER_PATH)\n",
    "    if next_day_prediction is not None:\n",
    "        last_close = data['Close'].iloc[-1]\n",
    "        print(f\"\\nLast available closing price ({data.index[-1].strftime('%Y-%m-%d')}): ${last_close:.2f}\")\n",
    "        print(f\"üìà Predicted closing price for the next trading day for {STOCK_TICKER}: ${next_day_prediction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"section11\"></a>\n",
    "## 11. Advanced Topics & Future Work\n",
    "\n",
    "This section explores advanced concepts and future development directions.\n",
    "\n",
    "### Sentiment Analysis Integration\n",
    "Research has shown that integrating sentiment scores from financial news can improve model accuracy. A full implementation would require:\n",
    "1.  **Data Acquisition:** Using a news API (e.g., NewsAPI, Bloomberg, Reuters) to fetch real-time text data.\n",
    "2.  **Temporal Alignment:** Calculating and aggregating sentiment scores (using a tool like VADER or a transformer model) for each trading day.\n",
    "3.  **Integration:** Adding the normalized sentiment score as an additional feature to the model's input `X`.\n",
    "\n",
    "### API Deployment for Real-time Inference\n",
    "For production use, the prediction function should be deployed as a RESTful API. This would allow trading platforms and dashboards to request predictions programmatically. A simple implementation could use a web framework like Flask or FastAPI.\n",
    "\n",
    "### Robustness: Fallback Data Sources\n",
    "For a mission-critical system, relying on a single data source is risky. A robust implementation would include fallback logic.\n",
    "\n",
    "```python\n",
    "def get_data_robust(ticker):\n",
    "    try:\n",
    "        # Try primary source\n",
    "        data = yf.download(ticker)\n",
    "        if data.empty: raise ValueError(\"No data from yfinance\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Primary source failed: {e}. Trying fallback...\")\n",
    "        # Try secondary source (e.g., Alpha Vantage, IEX Cloud)\n",
    "        # data = alpha_vantage.get_daily(...)\n",
    "        return data # or None if all fail\n",
    "```\n",
    "\n",
    "### Memory Management for Large Datasets\n",
    "While stock data is typically manageable, for extremely large datasets (e.g., tick-level data over decades), memory can be an issue.\n",
    "- **Data Types:** Use memory-efficient data types in pandas (e.g., `float32` instead of `float64`).\n",
    "- **Garbage Collection:** We use `del` and `gc.collect()` to manually free up memory after variables are no longer needed.\n",
    "- **Generators/tf.data:** For datasets that don't fit in memory, use Python generators or the `tf.data` API to feed data to the model in batches without loading everything at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Notebook Complete!\n",
    "\n",
    "You have successfully implemented a professional-grade LSTM model for stock price prediction. The model architecture and parameters provide a strong foundation for financial time-series forecasting.\n",
    "\n",
    "### Key Achievements:\n",
    "- ‚úÖ Built a stacked LSTM model with customizable parameters\n",
    "- ‚úÖ Implemented a robust data preprocessing and feature engineering pipeline\n",
    "- ‚úÖ Established a naive baseline for meaningful model validation\n",
    "- ‚úÖ Implemented model and scaler saving/loading for reliable, stateless predictions\n",
    "- ‚úÖ Created live prediction functionality for real-time inference\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment with different stocks** by changing the `STOCK_TICKER` parameter.\n",
    "2. **Tune hyperparameters** or **add more features** (e.g., sentiment scores) to potentially improve accuracy.\n",
    "3. **Deploy the model** as an API for production use.\n",
    "4. **Explore ensemble methods**, such as combining LSTM with other models like GRU or ARIMA.\n",
    "\n",
    "**Disclaimer:** Stock market prediction is inherently uncertain. This model is an educational tool and should be used to inform decisions, not as a guarantee of future performance. Always perform your own research and due diligence."
   ]
  }
 ]
}